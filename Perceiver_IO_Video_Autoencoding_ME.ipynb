{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hVqbkgBFVKB"
      },
      "outputs": [],
      "source": [
        "# Copyright 2021 DeepMind Technologies Limited\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd perceiver/train/\n"
      ],
      "metadata": {
        "id": "l70lrHbvzNn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ../"
      ],
      "metadata": {
        "id": "NlntzNEg0ca9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sm0mo9ciwhKu"
      },
      "outputs": [],
      "source": [
        "# Install dependencies for Google Colab.\n",
        "# If you want to run this notebook on your own machine, you can skip this cell.\n",
        "!pip install dm-haiku\n",
        "!pip install einops\n",
        "\n",
        "!mkdir /content/perceiver\n",
        "!touch /content/perceiver/__init__.py\n",
        "!wget -O /content/perceiver/io_processors.py https://raw.githubusercontent.com/deepmind/deepmind-research/master/perceiver/io_processors.py\n",
        "!wget -O /content/perceiver/perceiver.py https://raw.githubusercontent.com/deepmind/deepmind-research/master/perceiver/perceiver.py\n",
        "!wget -O /content/perceiver/position_encoding.py https://raw.githubusercontent.com/deepmind/deepmind-research/master/perceiver/position_encoding.py\n",
        "\n",
        "%cd perceiver/\n",
        "!mkdir train\n",
        "!wget -O /content/perceiver/train/launch_local.sh https://raw.githubusercontent.com/deepmind/deepmind-research/master/perceiver/train/launch_local.sh\n",
        "!wget -O /content/perceiver/train/autoaugment.py https://raw.githubusercontent.com/deepmind/deepmind-research/master/perceiver/train/autoaugment.py\n",
        "!wget -O /content/perceiver/train/dataset.py https://raw.githubusercontent.com/deepmind/deepmind-research/master/perceiver/train/dataset.py\n",
        "!wget -O /content/perceiver/train/experiment.py https://raw.githubusercontent.com/deepmind/deepmind-research/master/perceiver/train/experiment.py\n",
        "!wget -O /content/perceiver/train/utils.py https://raw.githubusercontent.com/deepmind/deepmind-research/master/perceiver/train/utils.py\n",
        "%cd ../"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHzUTH5KqNEt"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "\n",
        "import base64\n",
        "import functools\n",
        "import os\n",
        "import pickle\n",
        "import ssl\n",
        "import re\n",
        "import tempfile\n",
        "\n",
        "from urllib import request\n",
        "\n",
        "import cv2\n",
        "import haiku as hk\n",
        "import imageio\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import scipy.io.wavfile\n",
        "\n",
        "from IPython.display import HTML\n",
        "\n",
        "from perceiver import perceiver, io_processors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bn1jTwkv3gHf"
      },
      "outputs": [],
      "source": [
        "#@title Helper functions for the UCF101 dataset\n",
        "\n",
        "# Utilities to fetch videos from UCF101 dataset\n",
        "UCF_ROOT = 'https://www.crcv.ucf.edu/THUMOS14/UCF101/UCF101/'\n",
        "_VIDEO_LIST = None\n",
        "_CACHE_DIR = tempfile.mkdtemp()\n",
        "# As of July 2020, crcv.ucf.edu doesn't use a certificate accepted by the\n",
        "# default Colab environment anymore.\n",
        "unverified_context = ssl._create_unverified_context()\n",
        "\n",
        "def list_ucf_videos():\n",
        "  \"\"\"Lists videos available in UCF101 dataset.\"\"\"\n",
        "  global _VIDEO_LIST\n",
        "  if not _VIDEO_LIST:\n",
        "    index = request.urlopen(UCF_ROOT, context=unverified_context).read().decode('utf-8')\n",
        "    videos = re.findall('(v_[\\w_]+\\.avi)', index)\n",
        "    _VIDEO_LIST = sorted(set(videos))\n",
        "  return list(_VIDEO_LIST)\n",
        "\n",
        "def fetch_ucf_video(video):\n",
        "  \"\"\"Fetchs a video and cache into local filesystem.\"\"\"\n",
        "  cache_path = os.path.join(_CACHE_DIR, video)\n",
        "  if not os.path.exists(cache_path):\n",
        "    urlpath = request.urljoin(UCF_ROOT, video)\n",
        "    print('Fetching %s => %s' % (urlpath, cache_path))\n",
        "    data = request.urlopen(urlpath, context=unverified_context).read()\n",
        "    open(cache_path, \"wb\").write(data)\n",
        "  return cache_path\n",
        "\n",
        "# Utilities to open video files using CV2\n",
        "def crop_center_square(frame):\n",
        "  y, x = frame.shape[0:2]\n",
        "  min_dim = min(y, x)\n",
        "  start_x = (x // 2) - (min_dim // 2)\n",
        "  start_y = (y // 2) - (min_dim // 2)\n",
        "  return frame[start_y:start_y+min_dim,start_x:start_x+min_dim]\n",
        "\n",
        "def load_video(path, max_frames=0, resize=(224, 224)):\n",
        "  cap = cv2.VideoCapture(path)\n",
        "  frames = []\n",
        "  try:\n",
        "    while True:\n",
        "      ret, frame = cap.read()\n",
        "      if not ret:\n",
        "        break\n",
        "      frame = crop_center_square(frame)\n",
        "      frame = cv2.resize(frame, resize)\n",
        "      frame = frame[:, :, [2, 1, 0]]\n",
        "      frames.append(frame)\n",
        "      \n",
        "      if len(frames) == max_frames:\n",
        "        break\n",
        "  finally:\n",
        "    cap.release()\n",
        "  return np.array(frames) / 255.0\n",
        "\n",
        "def to_gif(images):\n",
        "  converted_images = np.clip(images * 255, 0, 255).astype(np.uint8)\n",
        "  imageio.mimsave('./animation.gif', converted_images, fps=25)\n",
        "  with open('./animation.gif', 'rb') as f:\n",
        "    gif_64 = base64.b64encode(f.read()).decode('utf-8')\n",
        "  return HTML('<img src=\"data:image/gif;base64,%s\"/>' % gif_64)\n",
        "\n",
        "def play_audio(data, sample_rate=48000):\n",
        "  scipy.io.wavfile.write('tmp_audio.wav', sample_rate, data)\n",
        "\n",
        "  with open('./tmp_audio.wav', 'rb') as f:\n",
        "    audio_64 = base64.b64encode(f.read()).decode('utf-8')\n",
        "  return HTML('<audio controls src=\"data:audio/wav;base64,%s\"/>' % audio_64)\n",
        "\n",
        "def table(elements):\n",
        "  row = ['<td>%s</td>' % el.data for el in elements]\n",
        "  return HTML('<table><tr>%s</tr></table>' % ''.join(row))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QqXUfdsF3iZ6"
      },
      "outputs": [],
      "source": [
        "#@title Load video and audio from UCF\n",
        "\n",
        "video_names = list_ucf_videos()\n",
        "video_path = fetch_ucf_video(video_names[0])\n",
        "\n",
        "# Extract audio using FFMPEG and encode as pcm float wavfile (only format readable by scipy.io.wavfile).\n",
        "!yes | ffmpeg -i \"$video_path\"  -c copy  -f wav -map 0:a pcm_f32le -ar 48000 output.wav\n",
        "\n",
        "sample_rate, audio = scipy.io.wavfile.read(\"output.wav\")\n",
        "if audio.dtype == np.int16:\n",
        "  audio = audio.astype(np.float32) / 2**15\n",
        "elif audio.dtype != np.float32:\n",
        "  raise ValueError('Unexpected datatype. Model expects sound samples to lie in [-1, 1]')\n",
        "\n",
        "video = load_video(video_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "6hxYUvnpqD8Y"
      },
      "outputs": [],
      "source": [
        "#@title Kinetics 700 Classes\n",
        "KINETICS_CLASSES = [\"abseiling\", \"acting in play\", \"adjusting glasses\", \"air drumming\", \n",
        "\"alligator wrestling\", \"answering questions\", \"applauding\", \"applying cream\", \n",
        "\"archaeological excavation\", \"archery\", \"arguing\", \"arm wrestling\", \n",
        "\"arranging flowers\", \"arresting\", \"assembling bicycle\", \"assembling computer\", \n",
        "\"attending conference\", \"auctioning\", \"baby waking up\", \"backflip (human)\", \n",
        "\"baking cookies\", \"bandaging\", \"barbequing\", \"bartending\", \n",
        "\"base jumping\", \"bathing dog\", \"battle rope training\", \"beatboxing\", \n",
        "\"bee keeping\", \"being excited\", \"being in zero gravity\", \"belly dancing\", \n",
        "\"bench pressing\", \"bending back\", \"bending metal\", \"biking through snow\", \n",
        "\"blasting sand\", \"blending fruit\", \"blowdrying hair\", \"blowing bubble gum\", \n",
        "\"blowing glass\", \"blowing leaves\", \"blowing nose\", \"blowing out candles\", \n",
        "\"bobsledding\", \"bodysurfing\", \"bookbinding\", \"bottling\", \n",
        "\"bouncing ball (not juggling)\", \"bouncing on bouncy castle\", \"bouncing on trampoline\", \"bowling\", \n",
        "\"braiding hair\", \"breading or breadcrumbing\", \"breakdancing\", \"breaking boards\", \n",
        "\"breaking glass\", \"breathing fire\", \"brush painting\", \"brushing floor\", \n",
        "\"brushing hair\", \"brushing teeth\", \"building cabinet\", \"building lego\", \n",
        "\"building sandcastle\", \"building shed\", \"bulldozing\", \"bungee jumping\", \n",
        "\"burping\", \"busking\", \"calculating\", \"calligraphy\", \n",
        "\"canoeing or kayaking\", \"capoeira\", \"capsizing\", \"card stacking\", \n",
        "\"card throwing\", \"carrying baby\", \"carrying weight\", \"cartwheeling\", \n",
        "\"carving ice\", \"carving marble\", \"carving pumpkin\", \"carving wood with a knife\", \n",
        "\"casting fishing line\", \"catching fish\", \"catching or throwing baseball\", \"catching or throwing frisbee\", \n",
        "\"catching or throwing softball\", \"celebrating\", \"changing gear in car\", \"changing oil\", \n",
        "\"changing wheel (not on bike)\", \"chasing\", \"checking tires\", \"checking watch\", \n",
        "\"cheerleading\", \"chewing gum\", \"chiseling stone\", \"chiseling wood\", \n",
        "\"chopping meat\", \"chopping wood\", \"clam digging\", \"clapping\", \n",
        "\"clay pottery making\", \"clean and jerk\", \"cleaning gutters\", \"cleaning pool\", \n",
        "\"cleaning shoes\", \"cleaning toilet\", \"cleaning windows\", \"climbing a rope\", \n",
        "\"climbing ladder\", \"climbing tree\", \"closing door\", \"coloring in\", \n",
        "\"combing hair\", \"contact juggling\", \"contorting\", \"cooking chicken\", \n",
        "\"cooking egg\", \"cooking on campfire\", \"cooking sausages (not on barbeque)\", \"cooking scallops\", \n",
        "\"cosplaying\", \"coughing\", \"counting money\", \"country line dancing\", \n",
        "\"cracking back\", \"cracking knuckles\", \"cracking neck\", \"crawling baby\", \n",
        "\"crocheting\", \"crossing eyes\", \"crossing river\", \"crying\", \n",
        "\"cumbia\", \"curling (sport)\", \"curling eyelashes\", \"curling hair\", \n",
        "\"cutting apple\", \"cutting cake\", \"cutting nails\", \"cutting orange\", \n",
        "\"cutting pineapple\", \"cutting watermelon\", \"dancing ballet\", \"dancing charleston\", \n",
        "\"dancing gangnam style\", \"dancing macarena\", \"deadlifting\", \"dealing cards\", \n",
        "\"decorating the christmas tree\", \"decoupage\", \"delivering mail\", \"digging\", \n",
        "\"dining\", \"directing traffic\", \"disc golfing\", \"diving cliff\", \n",
        "\"docking boat\", \"dodgeball\", \"doing aerobics\", \"doing jigsaw puzzle\", \n",
        "\"doing laundry\", \"doing nails\", \"doing sudoku\", \"drawing\", \n",
        "\"dribbling basketball\", \"drinking shots\", \"driving car\", \"driving tractor\", \n",
        "\"drooling\", \"drop kicking\", \"drumming fingers\", \"dumpster diving\", \n",
        "\"dunking basketball\", \"dyeing eyebrows\", \"dyeing hair\", \"eating burger\", \n",
        "\"eating cake\", \"eating carrots\", \"eating chips\", \"eating doughnuts\", \n",
        "\"eating hotdog\", \"eating ice cream\", \"eating nachos\", \"eating spaghetti\", \n",
        "\"eating watermelon\", \"egg hunting\", \"embroidering\", \"entering church\", \n",
        "\"exercising arm\", \"exercising with an exercise ball\", \"extinguishing fire\", \"faceplanting\", \n",
        "\"falling off bike\", \"falling off chair\", \"feeding birds\", \"feeding fish\", \n",
        "\"feeding goats\", \"fencing (sport)\", \"fidgeting\", \"filling cake\", \n",
        "\"filling eyebrows\", \"finger snapping\", \"fixing bicycle\", \"fixing hair\", \n",
        "\"flint knapping\", \"flipping bottle\", \"flipping pancake\", \"fly tying\", \n",
        "\"flying kite\", \"folding clothes\", \"folding napkins\", \"folding paper\", \n",
        "\"front raises\", \"frying vegetables\", \"gargling\", \"geocaching\", \n",
        "\"getting a haircut\", \"getting a piercing\", \"getting a tattoo\", \"giving or receiving award\", \n",
        "\"gold panning\", \"golf chipping\", \"golf driving\", \"golf putting\", \n",
        "\"gospel singing in church\", \"grinding meat\", \"grooming cat\", \"grooming dog\", \n",
        "\"grooming horse\", \"gymnastics tumbling\", \"hammer throw\", \"hand washing clothes\", \n",
        "\"head stand\", \"headbanging\", \"headbutting\", \"helmet diving\", \n",
        "\"herding cattle\", \"high fiving\", \"high jump\", \"high kick\", \n",
        "\"historical reenactment\", \"hitting baseball\", \"hockey stop\", \"holding snake\", \n",
        "\"home roasting coffee\", \"hopscotch\", \"hoverboarding\", \"huddling\", \n",
        "\"hugging (not baby)\", \"hugging baby\", \"hula hooping\", \"hurdling\", \n",
        "\"hurling (sport)\", \"ice climbing\", \"ice fishing\", \"ice skating\", \n",
        "\"ice swimming\", \"inflating balloons\", \"installing carpet\", \"ironing\", \n",
        "\"ironing hair\", \"javelin throw\", \"jaywalking\", \"jetskiing\", \n",
        "\"jogging\", \"juggling balls\", \"juggling fire\", \"juggling soccer ball\", \n",
        "\"jumping bicycle\", \"jumping into pool\", \"jumping jacks\", \"jumping sofa\", \n",
        "\"jumpstyle dancing\", \"karaoke\", \"kicking field goal\", \"kicking soccer ball\", \n",
        "\"kissing\", \"kitesurfing\", \"knitting\", \"krumping\", \n",
        "\"land sailing\", \"laughing\", \"lawn mower racing\", \"laying bricks\", \n",
        "\"laying concrete\", \"laying decking\", \"laying stone\", \"laying tiles\", \n",
        "\"leatherworking\", \"letting go of balloon\", \"licking\", \"lifting hat\", \n",
        "\"lighting candle\", \"lighting fire\", \"listening with headphones\", \"lock picking\", \n",
        "\"long jump\", \"longboarding\", \"looking at phone\", \"looking in mirror\", \n",
        "\"luge\", \"lunge\", \"making a cake\", \"making a sandwich\", \n",
        "\"making balloon shapes\", \"making bubbles\", \"making cheese\", \"making horseshoes\", \n",
        "\"making jewelry\", \"making latte art\", \"making paper aeroplanes\", \"making pizza\", \n",
        "\"making slime\", \"making snowman\", \"making sushi\", \"making tea\", \n",
        "\"making the bed\", \"marching\", \"marriage proposal\", \"massaging back\", \n",
        "\"massaging feet\", \"massaging legs\", \"massaging neck\", \"massaging person's head\", \n",
        "\"metal detecting\", \"milking cow\", \"milking goat\", \"mixing colours\", \n",
        "\"moon walking\", \"mopping floor\", \"mosh pit dancing\", \"motorcycling\", \n",
        "\"mountain climber (exercise)\", \"moving baby\", \"moving child\", \"moving furniture\", \n",
        "\"mowing lawn\", \"mushroom foraging\", \"needle felting\", \"news anchoring\", \n",
        "\"opening bottle (not wine)\", \"opening coconuts\", \"opening door\", \"opening present\", \n",
        "\"opening refrigerator\", \"opening wine bottle\", \"packing\", \"paragliding\", \n",
        "\"parasailing\", \"parkour\", \"passing American football (in game)\", \"passing American football (not in game)\", \n",
        "\"passing soccer ball\", \"peeling apples\", \"peeling banana\", \"peeling potatoes\", \n",
        "\"person collecting garbage\", \"petting animal (not cat)\", \"petting cat\", \"petting horse\", \n",
        "\"photobombing\", \"photocopying\", \"picking apples\", \"picking blueberries\", \n",
        "\"pillow fight\", \"pinching\", \"pirouetting\", \"planing wood\", \n",
        "\"planting trees\", \"plastering\", \"playing accordion\", \"playing american football\", \n",
        "\"playing badminton\", \"playing bagpipes\", \"playing basketball\", \"playing bass guitar\", \n",
        "\"playing beer pong\", \"playing billiards\", \"playing blackjack\", \"playing cards\", \n",
        "\"playing cello\", \"playing checkers\", \"playing chess\", \"playing clarinet\", \n",
        "\"playing controller\", \"playing cricket\", \"playing cymbals\", \"playing darts\", \n",
        "\"playing didgeridoo\", \"playing dominoes\", \"playing drums\", \"playing field hockey\", \n",
        "\"playing flute\", \"playing gong\", \"playing guitar\", \"playing hand clapping games\", \n",
        "\"playing harmonica\", \"playing harp\", \"playing ice hockey\", \"playing keyboard\", \n",
        "\"playing kickball\", \"playing laser tag\", \"playing lute\", \"playing mahjong\", \n",
        "\"playing maracas\", \"playing marbles\", \"playing monopoly\", \"playing netball\", \n",
        "\"playing nose flute\", \"playing oboe\", \"playing ocarina\", \"playing organ\", \n",
        "\"playing paintball\", \"playing pan pipes\", \"playing piano\", \"playing piccolo\", \n",
        "\"playing pinball\", \"playing ping pong\", \"playing poker\", \"playing polo\", \n",
        "\"playing recorder\", \"playing road hockey\", \"playing rounders\", \"playing rubiks cube\", \n",
        "\"playing saxophone\", \"playing scrabble\", \"playing shuffleboard\", \"playing slot machine\", \n",
        "\"playing squash or racquetball\", \"playing tennis\", \"playing trombone\", \"playing trumpet\", \n",
        "\"playing ukulele\", \"playing violin\", \"playing volleyball\", \"playing with trains\", \n",
        "\"playing xylophone\", \"poaching eggs\", \"poking bellybutton\", \"pole vault\", \n",
        "\"polishing furniture\", \"polishing metal\", \"popping balloons\", \"pouring beer\", \n",
        "\"pouring milk\", \"pouring wine\", \"preparing salad\", \"presenting weather forecast\", \n",
        "\"pretending to be a statue\", \"pull ups\", \"pulling espresso shot\", \"pulling rope (game)\", \n",
        "\"pumping fist\", \"pumping gas\", \"punching bag\", \"punching person (boxing)\", \n",
        "\"push up\", \"pushing car\", \"pushing cart\", \"pushing wheelbarrow\", \n",
        "\"pushing wheelchair\", \"putting in contact lenses\", \"putting on eyeliner\", \"putting on foundation\", \n",
        "\"putting on lipstick\", \"putting on mascara\", \"putting on sari\", \"putting on shoes\", \n",
        "\"putting wallpaper on wall\", \"raising eyebrows\", \"reading book\", \"reading newspaper\", \n",
        "\"recording music\", \"repairing puncture\", \"riding a bike\", \"riding camel\", \n",
        "\"riding elephant\", \"riding mechanical bull\", \"riding mule\", \"riding or walking with horse\", \n",
        "\"riding scooter\", \"riding snow blower\", \"riding unicycle\", \"ripping paper\", \n",
        "\"roasting marshmallows\", \"roasting pig\", \"robot dancing\", \"rock climbing\", \n",
        "\"rock scissors paper\", \"roller skating\", \"rolling eyes\", \"rolling pastry\", \n",
        "\"rope pushdown\", \"running on treadmill\", \"sailing\", \"salsa dancing\", \n",
        "\"saluting\", \"sanding floor\", \"sanding wood\", \"sausage making\", \n",
        "\"sawing wood\", \"scrambling eggs\", \"scrapbooking\", \"scrubbing face\", \n",
        "\"scuba diving\", \"seasoning food\", \"separating eggs\", \"setting table\", \n",
        "\"sewing\", \"shaking hands\", \"shaking head\", \"shaping bread dough\", \n",
        "\"sharpening knives\", \"sharpening pencil\", \"shaving head\", \"shaving legs\", \n",
        "\"shearing sheep\", \"shining flashlight\", \"shining shoes\", \"shoot dance\", \n",
        "\"shooting basketball\", \"shooting goal (soccer)\", \"shooting off fireworks\", \"shopping\", \n",
        "\"shot put\", \"shouting\", \"shoveling snow\", \"shredding paper\", \n",
        "\"shucking oysters\", \"shuffling cards\", \"shuffling feet\", \"side kick\", \n",
        "\"sieving\", \"sign language interpreting\", \"silent disco\", \"singing\", \n",
        "\"sipping cup\", \"situp\", \"skateboarding\", \"ski ballet\", \n",
        "\"ski jumping\", \"skiing crosscountry\", \"skiing mono\", \"skiing slalom\", \n",
        "\"skipping rope\", \"skipping stone\", \"skydiving\", \"slacklining\", \n",
        "\"slapping\", \"sled dog racing\", \"sleeping\", \"slicing onion\", \n",
        "\"smashing\", \"smelling feet\", \"smoking\", \"smoking hookah\", \n",
        "\"smoking pipe\", \"snatch weight lifting\", \"sneezing\", \"snorkeling\", \n",
        "\"snowboarding\", \"snowkiting\", \"snowmobiling\", \"somersaulting\", \n",
        "\"spelunking\", \"spinning plates\", \"spinning poi\", \"splashing water\", \n",
        "\"spray painting\", \"spraying\", \"springboard diving\", \"square dancing\", \n",
        "\"squat\", \"squeezing orange\", \"stacking cups\", \"stacking dice\", \n",
        "\"standing on hands\", \"staring\", \"steer roping\", \"steering car\", \n",
        "\"sticking tongue out\", \"stomping grapes\", \"stretching arm\", \"stretching leg\", \n",
        "\"sucking lolly\", \"surfing crowd\", \"surfing water\", \"surveying\", \n",
        "\"sweeping floor\", \"swimming backstroke\", \"swimming breast stroke\", \"swimming butterfly stroke\", \n",
        "\"swimming front crawl\", \"swimming with dolphins\", \"swimming with sharks\", \"swing dancing\", \n",
        "\"swinging baseball bat\", \"swinging on something\", \"sword fighting\", \"sword swallowing\", \n",
        "\"tackling\", \"tagging graffiti\", \"tai chi\", \"taking photo\", \n",
        "\"talking on cell phone\", \"tango dancing\", \"tap dancing\", \"tapping guitar\", \n",
        "\"tapping pen\", \"tasting beer\", \"tasting food\", \"tasting wine\", \n",
        "\"testifying\", \"texting\", \"threading needle\", \"throwing axe\", \n",
        "\"throwing ball (not baseball or American football)\", \"throwing discus\", \"throwing knife\", \"throwing snowballs\", \n",
        "\"throwing tantrum\", \"throwing water balloon\", \"tickling\", \"tie dying\", \n",
        "\"tightrope walking\", \"tiptoeing\", \"tobogganing\", \"tossing coin\", \n",
        "\"tossing salad\", \"training dog\", \"trapezing\", \"treating wood\", \n",
        "\"trimming or shaving beard\", \"trimming shrubs\", \"trimming trees\", \"triple jump\", \n",
        "\"twiddling fingers\", \"tying bow tie\", \"tying knot (not on a tie)\", \"tying necktie\", \n",
        "\"tying shoe laces\", \"unboxing\", \"uncorking champagne\", \"unloading truck\", \n",
        "\"using a microscope\", \"using a paint roller\", \"using a power drill\", \"using a sledge hammer\", \n",
        "\"using a wrench\", \"using atm\", \"using bagging machine\", \"using circular saw\", \n",
        "\"using inhaler\", \"using megaphone\", \"using puppets\", \"using remote controller (not gaming)\", \n",
        "\"using segway\", \"vacuuming car\", \"vacuuming floor\", \"visiting the zoo\", \n",
        "\"wading through mud\", \"wading through water\", \"waiting in line\", \"waking up\", \n",
        "\"walking on stilts\", \"walking the dog\", \"walking through snow\", \"walking with crutches\", \n",
        "\"washing dishes\", \"washing feet\", \"washing hair\", \"washing hands\", \n",
        "\"watching tv\", \"water skiing\", \"water sliding\", \"watering plants\", \n",
        "\"waving hand\", \"waxing armpits\", \"waxing back\", \"waxing chest\", \n",
        "\"waxing eyebrows\", \"waxing legs\", \"weaving basket\", \"weaving fabric\", \n",
        "\"welding\", \"whistling\", \"windsurfing\", \"winking\", \n",
        "\"wood burning (art)\", \"wrapping present\", \"wrestling\", \"writing\", \n",
        "\"yarn spinning\", \"yawning\", \"yoga\", \"zumba\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQpSe7DMhuln"
      },
      "outputs": [],
      "source": [
        "# Visualize inputs\n",
        "table([to_gif(video), play_audio(audio)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxeP5yit7hJg"
      },
      "outputs": [],
      "source": [
        "#@title Model construction\n",
        "NUM_FRAMES = 16\n",
        "AUDIO_SAMPLES_PER_FRAME = 48000 // 25\n",
        "SAMPLES_PER_PATCH = 16\n",
        "NUM_CLASSES = 700\n",
        "IMG_SZ = 56\n",
        "\n",
        "def video_autoencoder(images, audio, subsampling):\n",
        "  n_audio_samples = NUM_FRAMES * AUDIO_SAMPLES_PER_FRAME\n",
        "  input_preprocessor = io_processors.MultimodalPreprocessor(\n",
        "      min_padding_size=4,\n",
        "      modalities={\n",
        "          'audio': io_processors.AudioPreprocessor(\n",
        "              position_encoding_type='fourier',\n",
        "              fourier_position_encoding_kwargs=dict(\n",
        "                  num_bands=192,\n",
        "                  max_resolution=(n_audio_samples,),\n",
        "                  sine_only=False,\n",
        "                  concat_pos=True,\n",
        "              ),\n",
        "              n_extra_pos_mlp=0,\n",
        "              prep_type='patches',\n",
        "              samples_per_patch=16),\n",
        "          'image': io_processors.ImagePreprocessor(\n",
        "              position_encoding_type='fourier',\n",
        "              fourier_position_encoding_kwargs=dict(\n",
        "                  num_bands=32,\n",
        "                  max_resolution=(NUM_FRAMES, IMG_SZ, IMG_SZ),\n",
        "                  sine_only=False,\n",
        "                  concat_pos=True,\n",
        "              ),\n",
        "              n_extra_pos_mlp=0,\n",
        "              prep_type='patches',\n",
        "              spatial_downsample=4,\n",
        "              temporal_downsample=1),\n",
        "          'label': io_processors.OneHotPreprocessor(),\n",
        "      },\n",
        "      mask_probs={'image': 0.0, 'audio': 0.0, 'label': 1.0},\n",
        "  )\n",
        "\n",
        "  output_postprocessor = io_processors.MultimodalPostprocessor(\n",
        "      modalities={\n",
        "          'audio': io_processors.AudioPostprocessor(\n",
        "              samples_per_patch=SAMPLES_PER_PATCH),\n",
        "          'image': io_processors.ProjectionPostprocessor(\n",
        "              num_outputs=3),\n",
        "          'label': io_processors.ClassificationPostprocessor(\n",
        "              num_classes=NUM_CLASSES),\n",
        "      })\n",
        "\n",
        "  encoder = encoder = perceiver.PerceiverEncoder(\n",
        "      num_self_attends_per_block=8,\n",
        "      # Weights won't be shared if num_blocks is set to 1.\n",
        "      num_blocks=1,\n",
        "      z_index_dim=28*28*1,\n",
        "      num_z_channels=512,\n",
        "      num_cross_attend_heads=1,\n",
        "      num_self_attend_heads=8,\n",
        "      cross_attend_widening_factor=1,\n",
        "      self_attend_widening_factor=1,\n",
        "      dropout_prob=0.0,\n",
        "      z_pos_enc_init_scale=0.02,\n",
        "      cross_attention_shape_for_attn='kv',\n",
        "      name='encoder')\n",
        "\n",
        "  subsampled_index_dims = {\n",
        "      'audio': subsampling['audio'].shape[0],\n",
        "      'image': subsampling['image'].shape[0],\n",
        "      'label': 1,\n",
        "  }\n",
        "  image_decoder = perceiver.BasicVideoAutoencodingDecoder(\n",
        "      # Autoencoding, don't pass inputs to the queries.\n",
        "      concat_preprocessed_input=False,\n",
        "      subsampled_index_dims=subsampling['image'],\n",
        "      output_shape=images.shape[:4],\n",
        "      num_z_channels=1024,\n",
        "      output_num_channels=512,\n",
        "      use_query_residual=False,\n",
        "      position_encoding_type='fourier',\n",
        "      fourier_position_encoding_kwargs=dict(\n",
        "          num_bands=32,\n",
        "          max_resolution=(NUM_FRAMES, IMG_SZ, IMG_SZ),\n",
        "          sine_only=False,\n",
        "          concat_pos=True,\n",
        "      ),\n",
        "  )\n",
        "\n",
        "  decoder = perceiver.MultimodalDecoder(\n",
        "      # Autoencoding, don't pass inputs to the queries.\n",
        "      concat_preprocessed_input=False,\n",
        "      subsampled_index_dims=subsampled_index_dims,\n",
        "      # Modality specific decoders are used ONLY to generate queries.\n",
        "      # All modalties are decoded together using a unified decoder.\n",
        "      modalities={\n",
        "          'audio': perceiver.BasicDecoder(\n",
        "              # Autoencoding, don't pass inputs to the queries.\n",
        "              concat_preprocessed_input=False,\n",
        "              subsampled_index_dims=subsampling['audio'],\n",
        "              output_index_dims=(n_audio_samples // SAMPLES_PER_PATCH,),\n",
        "              num_z_channels=1024,\n",
        "              output_num_channels=512,\n",
        "              use_query_residual=False,\n",
        "              position_encoding_type='fourier',\n",
        "              fourier_position_encoding_kwargs=dict(\n",
        "                  num_bands=192,\n",
        "                  max_resolution=(n_audio_samples,),\n",
        "                  sine_only=False,\n",
        "                  concat_pos=True,\n",
        "              ),\n",
        "           ),\n",
        "          'image': image_decoder,\n",
        "          'label': perceiver.ClassificationDecoder(\n",
        "              # Autoencoding, don't pass inputs to the queries.\n",
        "              concat_preprocessed_input=False,\n",
        "              num_classes=NUM_CLASSES,\n",
        "              num_z_channels=1024,\n",
        "              use_query_residual=False,\n",
        "              position_encoding_type='trainable',\n",
        "              trainable_position_encoding_kwargs=dict(\n",
        "                  num_channels=1024,\n",
        "                  init_scale=0.02,\n",
        "              ),\n",
        "          ),\n",
        "      },\n",
        "      num_outputs=None,\n",
        "      output_num_channels=512,\n",
        "      use_query_residual=False,)\n",
        "  \n",
        "  model = perceiver.Perceiver(\n",
        "      input_preprocessor=input_preprocessor,\n",
        "      encoder=encoder,\n",
        "      decoder=decoder,\n",
        "      output_postprocessor=output_postprocessor)\n",
        "  \n",
        "  return model({'image': images,\n",
        "                'audio': audio,\n",
        "                'label': np.zeros((images.shape[0], 700))},\n",
        "               is_training=False, subsampled_output_points=subsampling)\n",
        "\n",
        "\n",
        "video_autoencoder = hk.transform_with_state(video_autoencoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N88EZZ7QHvbu"
      },
      "outputs": [],
      "source": [
        "#@title Model application\n",
        "\n",
        "\n",
        "def autoencode_video(params, state, rng, images, audio):\n",
        "  nchunks = 128\n",
        "  reconstruction = {}\n",
        "  for chunk_idx in range(nchunks):\n",
        "    image_chunk_size = np.prod(images.shape[1:-1]) // nchunks\n",
        "    audio_chunk_size = audio.shape[1] // SAMPLES_PER_PATCH // nchunks\n",
        "    subsampling = {\n",
        "        'image': jnp.arange(\n",
        "            image_chunk_size * chunk_idx, image_chunk_size * (chunk_idx + 1)),\n",
        "        'audio': jnp.arange(\n",
        "            audio_chunk_size * chunk_idx, audio_chunk_size * (chunk_idx + 1)),\n",
        "        'label': None,\n",
        "    }\n",
        "    output, state = video_autoencoder.apply(\n",
        "        params, state, rng, images, audio, subsampling)\n",
        "    reconstruction['label'] = output['label']\n",
        "    if 'image' not in reconstruction:\n",
        "      reconstruction['image'] = output['image']\n",
        "      reconstruction['audio'] = output['audio']\n",
        "    else:\n",
        "      reconstruction['image'] = jnp.concatenate(\n",
        "          [reconstruction['image'], output['image']], axis=1)\n",
        "      reconstruction['audio'] = jnp.concatenate(\n",
        "          [reconstruction['audio'], output['audio']], axis=1)\n",
        "      \n",
        "  reconstruction['image'] = jnp.reshape(reconstruction['image'], images.shape)\n",
        "  reconstruction['audio'] = jnp.reshape(reconstruction['audio'], audio.shape)\n",
        "  return reconstruction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVRWatw4LXFx"
      },
      "outputs": [],
      "source": [
        "#@title Load parameters from checkpoint\n",
        "\n",
        "!wget -O video_autoencoding_checkpoint.pystate https://storage.googleapis.com/perceiver_io/video_autoencoding_checkpoint.pystate\n",
        "\n",
        "rng = jax.random.PRNGKey(42)\n",
        "with open(\"video_autoencoding_checkpoint.pystate\", \"rb\") as f:\n",
        "  params = pickle.loads(f.read())\n",
        "\n",
        "state = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfWpBNZJLib4"
      },
      "outputs": [],
      "source": [
        "# Auto-encode the first 16 frames of the video and one of the audio channels\n",
        "reconstruction = autoencode_video(params, state, rng, video[None, :16], audio[None, :16*AUDIO_SAMPLES_PER_FRAME, 0:1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZpggBTO4eO5"
      },
      "outputs": [],
      "source": [
        "# Visualize reconstruction of first 16 frames\n",
        "table([to_gif(reconstruction[\"image\"][0]), play_audio(np.array(reconstruction[\"audio\"][0]))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oTJzBsl6xkOP"
      },
      "outputs": [],
      "source": [
        "# Kinetics 700 Labels\n",
        "scores, indices = jax.lax.top_k(jax.nn.softmax(reconstruction[\"label\"]), 5)\n",
        "\n",
        "for score, index in zip(scores[0], indices[0]):\n",
        "  print(\"%s: %s\" % (KINETICS_CLASSES[index], score))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7JZRtQdwC4eE"
      },
      "outputs": [],
      "source": [
        "# Auto-encode the entire video, one chunk at a time\n",
        "\n",
        "# Partial video and audio into 16-frame chunks\n",
        "nframes = video.shape[0]\n",
        "# Truncate to be divisible by 16\n",
        "nframes = nframes - (nframes % 16)\n",
        "video_chunks = jnp.reshape(video[:nframes], [nframes // 16, 16, 224, 224, 3])\n",
        "audio_chunks = jnp.reshape(audio[:nframes * AUDIO_SAMPLES_PER_FRAME],\n",
        "                           [nframes // 16, 16 * AUDIO_SAMPLES_PER_FRAME, 2])\n",
        "\n",
        "encode = jax.jit(functools.partial(autoencode_video, params, state, rng))\n",
        "\n",
        "# Logically, what we do is the following code. We write out the loop to allocate\n",
        "# GPU memory for only one chunk\n",
        "#\n",
        "# reconstruction = jax.vmap(encode, in_axes=1, out_axes=1)(\n",
        "#     video_chunks[None, :], audio_chunks[None, :, :, 0:1])\n",
        "\n",
        "chunks = []\n",
        "for i in range(nframes // 16):\n",
        "  reconstruction = encode(video_chunks[None, i], audio_chunks[None, i, :, 0:1])\n",
        "  chunks.append(jax.tree_map(lambda x: np.array(x), reconstruction))\n",
        "\n",
        "reconstruction = jax.tree_multimap(lambda *args: np.stack(args, axis=1),\n",
        "                                   *chunks)\n",
        "\n",
        "reconstruction = jax.tree_map(lambda x: np.reshape(x, [-1] + list(x.shape[2:])), reconstruction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzwbz1mgES4d"
      },
      "outputs": [],
      "source": [
        "# Visualize reconstruction of entire video\n",
        "table([to_gif(reconstruction['image'][0]), play_audio(np.array(reconstruction['audio'][0]))])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jaxline\n",
        "!pip install optax"
      ],
      "metadata": {
        "id": "NelrNlV8yvBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-addons \n",
        "!pip install tensorflow-probability tensorflow-datasets"
      ],
      "metadata": {
        "id": "FtxvvYGj3eYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_dJY-QEA3eSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright 2021 DeepMind Technologies Limited\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "\"\"\"A reference training pipeline for Perceiver/Perceiver IO on ImageNet.\n",
        "\n",
        "We use the Jaxline (https://github.com/deepmind/jaxline) training framework.\n",
        "Two sets of hyperparameters are provided, the hyperparameters we used for the\n",
        "Perceiver IO paper, and scaled-down hyperparameters for local testing.\n",
        "This script should run out-of-the-box with the local hyper parameters.\n",
        "The scaled-up hyperparameters requires a distributed learning setup to run,\n",
        "and this script will need to be adapted to your specific setup.\n",
        "\"\"\"\n",
        "\n",
        "import functools\n",
        "from typing import Generator, Mapping, Text, Tuple\n",
        "\n",
        "from absl import app\n",
        "from absl import flags\n",
        "from absl import logging\n",
        "import haiku as hk\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jaxline import base_config\n",
        "from jaxline import experiment\n",
        "from jaxline import platform\n",
        "from jaxline import utils as jl_utils\n",
        "from ml_collections import config_dict\n",
        "import numpy as np\n",
        "import optax\n",
        "\n",
        "\n",
        "import io_processors\n",
        "import perceiver\n",
        "\n",
        "\n",
        "from train import dataset\n",
        "from train import utils\n",
        "\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "OptState = Tuple[optax.TraceState, optax.ScaleByScheduleState, optax.ScaleState]\n",
        "Scalars = Mapping[Text, jnp.ndarray]\n",
        "\n",
        "\n",
        "N_TRAIN_EXAMPLES = dataset.Split.TRAIN_AND_VALID.num_examples\n",
        "N_CLASSES = 1000\n",
        "# Only local/debug parameters are supported out of the box.\n",
        "# To use the scaled-up hyperparameters, please adapt this script to your\n",
        "# training setup and set this flag to False\n",
        "IS_LOCAL = True\n",
        "\n",
        "\n",
        "def get_training_steps(batch_size, n_epochs):\n",
        "  return (N_TRAIN_EXAMPLES * n_epochs) // batch_size\n",
        "\n",
        "\n",
        "def get_config():\n",
        "  \"\"\"Return config object for training.\"\"\"\n",
        "  use_debug_settings = IS_LOCAL\n",
        "  config = base_config.get_base_config()\n",
        "\n",
        "  # Experiment config.\n",
        "  local_batch_size = 2\n",
        "  # Modify this to adapt to your custom distributed learning setup\n",
        "  num_devices = 1\n",
        "  config.train_batch_size = local_batch_size * num_devices\n",
        "  config.n_epochs = 110\n",
        "\n",
        "  def _default_or_debug(default_value, debug_value):\n",
        "    return debug_value if use_debug_settings else default_value\n",
        "\n",
        "  n_train_examples = N_TRAIN_EXAMPLES\n",
        "  num_classes = N_CLASSES\n",
        "\n",
        "  config.experiment_kwargs = config_dict.ConfigDict(\n",
        "      dict(\n",
        "          config=dict(\n",
        "              optimizer=dict(\n",
        "                  base_lr=5e-4,\n",
        "                  max_norm=10.0,  # < 0 to turn off.\n",
        "                  schedule_type='constant_cosine',\n",
        "                  weight_decay=1e-1,\n",
        "                  decay_pos_embs=True,\n",
        "                  scale_by_batch=True,\n",
        "                  cosine_decay_kwargs=dict(\n",
        "                      init_value=0.0,\n",
        "                      warmup_epochs=0,\n",
        "                      end_value=0.0,\n",
        "                  ),\n",
        "                  step_decay_kwargs=dict(\n",
        "                      decay_boundaries=[0.5, 0.8, 0.95],\n",
        "                      decay_rate=0.1,\n",
        "                  ),\n",
        "                  constant_cosine_decay_kwargs=dict(\n",
        "                      constant_fraction=0.5,\n",
        "                      end_value=0.0,\n",
        "                  ),\n",
        "                  optimizer='lamb',\n",
        "                  # Optimizer-specific kwargs:\n",
        "                  adam_kwargs=dict(\n",
        "                      b1=0.9,\n",
        "                      b2=0.999,\n",
        "                      eps=1e-8,\n",
        "                  ),\n",
        "                  lamb_kwargs=dict(\n",
        "                      b1=0.9,\n",
        "                      b2=0.999,\n",
        "                      eps=1e-6,\n",
        "                  ),\n",
        "              ),\n",
        "              # Don't specify output_channels - it's not used for\n",
        "              # classifiers.\n",
        "              model=dict(\n",
        "                  perceiver_kwargs=dict(\n",
        "                      input_preprocessor=dict(\n",
        "                          prep_type='pixels',\n",
        "                          # Channels for conv/conv1x1 preprocessing:\n",
        "                          num_channels=64,\n",
        "                          # -------------------------\n",
        "                          # Position encoding arguments:\n",
        "                          # -------------------------\n",
        "                          position_encoding_type='fourier',\n",
        "                          concat_or_add_pos='concat',\n",
        "                          spatial_downsample=1,\n",
        "                          # If >0, project position to this size:\n",
        "                          project_pos_dim=-1,\n",
        "                          trainable_position_encoding_kwargs=dict(\n",
        "                              num_channels=258,  # Match default # for Fourier.\n",
        "                              init_scale=0.02,\n",
        "                          ),\n",
        "                          fourier_position_encoding_kwargs=dict(\n",
        "                              num_bands=64,\n",
        "                              max_resolution=(224, 224),\n",
        "                              sine_only=False,\n",
        "                              concat_pos=True,\n",
        "                          ),\n",
        "                      ),\n",
        "                      encoder=dict(\n",
        "                          num_self_attends_per_block=_default_or_debug(6, 2),\n",
        "                          # Weights won't be shared if num_blocks is set to 1.\n",
        "                          num_blocks=_default_or_debug(8, 2),\n",
        "                          z_index_dim=512,\n",
        "                          num_z_channels=1024,\n",
        "                          num_cross_attend_heads=1,\n",
        "                          num_self_attend_heads=8,\n",
        "                          cross_attend_widening_factor=1,\n",
        "                          self_attend_widening_factor=1,\n",
        "                          dropout_prob=0.0,\n",
        "                          # Position encoding for the latent array.\n",
        "                          z_pos_enc_init_scale=0.02,\n",
        "                          cross_attention_shape_for_attn='kv',\n",
        "                          use_query_residual=True,\n",
        "                          ),\n",
        "                      decoder=dict(\n",
        "                          num_z_channels=1024,\n",
        "                          use_query_residual=True,\n",
        "                          # Position encoding for the output logits.\n",
        "                          position_encoding_type='trainable',\n",
        "                          trainable_position_encoding_kwargs=dict(\n",
        "                              num_channels=1024,\n",
        "                              init_scale=0.02,\n",
        "                          ),\n",
        "                      ),\n",
        "                  ),\n",
        "              ),\n",
        "              training=dict(\n",
        "                  images_per_epoch=n_train_examples,\n",
        "                  label_smoothing=0.1,\n",
        "                  n_epochs=config.get_oneway_ref('n_epochs'),\n",
        "                  batch_size=config.get_oneway_ref('train_batch_size')\n",
        "              ),\n",
        "              data=dict(\n",
        "                  num_classes=num_classes,\n",
        "                  # Run on smaller images to debug.\n",
        "                  im_dim=_default_or_debug(224, 32),\n",
        "                  augmentation=dict(\n",
        "                      # Typical randaug params:\n",
        "                      # num_layers in [1, 3]\n",
        "                      # magnitude in [5, 30]\n",
        "                      # Set randaugment to None to disable.\n",
        "                      randaugment=dict(\n",
        "                          num_layers=4,\n",
        "                          magnitude=5),\n",
        "                      cutmix=True,\n",
        "                      # Mixup alpha should be in [0, 1].\n",
        "                      # Set to None to disable.\n",
        "                      mixup_alpha=0.2,\n",
        "                  ),\n",
        "                  ),\n",
        "              evaluation=dict(\n",
        "                  subset='test',\n",
        "                  batch_size=2,\n",
        "              ),\n",
        "          )\n",
        "      )\n",
        "  )\n",
        "\n",
        "  # Training loop config.\n",
        "  config.training_steps = get_training_steps(\n",
        "      config.get_oneway_ref('train_batch_size'),\n",
        "      config.get_oneway_ref('n_epochs'))\n",
        "  config.log_train_data_interval = 60\n",
        "  config.log_tensors_interval = 60\n",
        "  config.save_checkpoint_interval = 300\n",
        "  config.eval_specific_checkpoint_dir = ''\n",
        "  config.best_model_eval_metric = 'eval_top_1_acc'\n",
        "  config.checkpoint_dir = '/tmp/perceiver_imagnet_checkpoints'\n",
        "  config.train_checkpoint_all_hosts = False\n",
        "\n",
        "  # Prevents accidentally setting keys that aren't recognized (e.g. in tests).\n",
        "  config.lock()\n",
        "\n",
        "  return config\n",
        "\n",
        "\n",
        "class Experiment(experiment.AbstractExperiment):\n",
        "  \"\"\"ImageNet experiment.\"\"\"\n",
        "\n",
        "  # A map from object properties that will be checkpointed to their name\n",
        "  # in a checkpoint. Currently we assume that these are all sharded\n",
        "  # device arrays.\n",
        "  CHECKPOINT_ATTRS = {\n",
        "      '_params': 'params',\n",
        "      '_state': 'state',\n",
        "      '_opt_state': 'opt_state',\n",
        "  }\n",
        "\n",
        "  def __init__(self, mode, init_rng, config):\n",
        "    \"\"\"Initializes experiment.\"\"\"\n",
        "\n",
        "    super(Experiment, self).__init__(mode=mode, init_rng=init_rng)\n",
        "\n",
        "    self.mode = mode\n",
        "    self.init_rng = init_rng\n",
        "    self.config = config\n",
        "\n",
        "    # Checkpointed experiment state.\n",
        "    self._params = None\n",
        "    self._state = None\n",
        "    self._opt_state = None\n",
        "\n",
        "    # Input pipelines.\n",
        "    self._train_input = None\n",
        "    self._eval_input = None\n",
        "\n",
        "    self.forward = hk.transform_with_state(self._forward_fn)\n",
        "\n",
        "    # NOTE: We \"donate\" the `params, state, opt_state` arguments which allows\n",
        "    # JAX (on some backends) to reuse the device memory associated with these\n",
        "    # inputs to store the outputs of our function (which also start with\n",
        "    # `params, state, opt_state`).\n",
        "    self._update_func = jax.pmap(self._update_func, axis_name='i',\n",
        "                                 donate_argnums=(0, 1, 2))\n",
        "    self._eval_batch = jax.jit(self._eval_batch)\n",
        "\n",
        "  def _forward_fn(\n",
        "      self,\n",
        "      inputs: dataset.Batch,\n",
        "      is_training: bool,\n",
        "  ) -> jnp.ndarray:\n",
        "\n",
        "    images = inputs['images']\n",
        "\n",
        "    perceiver_kwargs = self.config.model.perceiver_kwargs\n",
        "    input_preprocessor = io_processors.ImagePreprocessor(\n",
        "        **perceiver_kwargs['input_preprocessor'])\n",
        "    encoder = perceiver.PerceiverEncoder(**perceiver_kwargs['encoder'])\n",
        "    decoder = perceiver.ClassificationDecoder(\n",
        "        self.config.data.num_classes,\n",
        "        **perceiver_kwargs['decoder'])\n",
        "    model = perceiver.Perceiver(\n",
        "        encoder=encoder,\n",
        "        decoder=decoder,\n",
        "        input_preprocessor=input_preprocessor)\n",
        "\n",
        "    return model(images, is_training=is_training)\n",
        "\n",
        "  #  _             _\n",
        "  # | |_ _ __ __ _(_)_ __\n",
        "  # | __| '__/ _` | | '_ \\\n",
        "  # | |_| | | (_| | | | | |\n",
        "  #  \\__|_|  \\__,_|_|_| |_|\n",
        "  #\n",
        "\n",
        "  def step(self, global_step: int, rng: jnp.ndarray,\n",
        "           *unused_args, **unused_kwargs):\n",
        "    \"\"\"See base class.\"\"\"\n",
        "\n",
        "    if self._train_input is None:\n",
        "      self._initialize_train()\n",
        "\n",
        "    inputs = next(self._train_input)\n",
        "\n",
        "    self._params, self._state, self._opt_state, scalars = (\n",
        "        self._update_func(\n",
        "            self._params, self._state, self._opt_state, inputs, rng, global_step\n",
        "            ))\n",
        "\n",
        "    scalars = jl_utils.get_first(scalars)\n",
        "    return scalars\n",
        "\n",
        "  def _initialize_train(self):\n",
        "    self._train_input = jl_utils.py_prefetch(self._build_train_input)\n",
        "\n",
        "    total_batch_size = self.config.training.batch_size\n",
        "    steps_per_epoch = (\n",
        "        self.config.training.images_per_epoch / self.config.training.batch_size)\n",
        "    total_steps = self.config.training.n_epochs * steps_per_epoch\n",
        "    # Scale by the (negative) learning rate.\n",
        "    self._lr_schedule = utils.get_learning_rate_schedule(\n",
        "        total_batch_size, steps_per_epoch, total_steps, self.config.optimizer)\n",
        "\n",
        "    self._optimizer = utils.make_optimizer(\n",
        "        self.config.optimizer,\n",
        "        self._lr_schedule)\n",
        "\n",
        "    # Check we haven't already restored params\n",
        "    if self._params is None:\n",
        "      logging.info('Initializing parameters.')\n",
        "\n",
        "      inputs = next(self._train_input)\n",
        "\n",
        "      init_net = jax.pmap(lambda *a: self.forward.init(*a, is_training=True))\n",
        "      init_opt = jax.pmap(self._optimizer.init)\n",
        "\n",
        "      # Init uses the same RNG key on all hosts+devices to ensure everyone\n",
        "      # computes the same initial state.\n",
        "      init_rng = jl_utils.bcast_local_devices(self.init_rng)\n",
        "\n",
        "      self._params, self._state = init_net(init_rng, inputs)\n",
        "      self._opt_state = init_opt(self._params)\n",
        "\n",
        "  def _load_data(self, split, is_training, batch_dims):\n",
        "    \"\"\"Wrapper for dataset loading.\"\"\"\n",
        "\n",
        "    return dataset.load(\n",
        "        split=split,\n",
        "        is_training=is_training,\n",
        "        batch_dims=batch_dims,\n",
        "        im_dim=self.config.data.im_dim,\n",
        "        augmentation_settings=self.config.data.augmentation,\n",
        "        )\n",
        "\n",
        "  def _build_train_input(self) -> Generator[dataset.Batch, None, None]:\n",
        "    \"\"\"See base class.\"\"\"\n",
        "    num_devices = jax.device_count()\n",
        "    global_batch_size = self.config.training.batch_size\n",
        "    per_device_batch_size, ragged = divmod(global_batch_size, num_devices)\n",
        "\n",
        "    if ragged:\n",
        "      raise ValueError(\n",
        "          'Global batch size {global_batch_size} must be divisible by '\n",
        "          'num devices {num_devices}')\n",
        "\n",
        "    split = dataset.Split.TRAIN_AND_VALID\n",
        "\n",
        "    return self._load_data(\n",
        "        split=split,\n",
        "        is_training=True,\n",
        "        batch_dims=[jax.local_device_count(), per_device_batch_size])\n",
        "\n",
        "  def _one_hot(self, value):\n",
        "    \"\"\"One-hot encoding potentially over a sequence of labels.\"\"\"\n",
        "    y = jax.nn.one_hot(value, self.config.data.num_classes)\n",
        "    return y\n",
        "\n",
        "  def _loss_fn(\n",
        "      self,\n",
        "      params: hk.Params,\n",
        "      state: hk.State,\n",
        "      inputs: dataset.Batch,\n",
        "      rng: jnp.ndarray,\n",
        "  ) -> Tuple[jnp.ndarray, Tuple[Scalars, hk.State]]:\n",
        "    logits, state = self.forward.apply(\n",
        "        params, state, rng, inputs, is_training=True)\n",
        "\n",
        "    label = self._one_hot(inputs['labels'])\n",
        "    # Handle cutmix/mixup label mixing:\n",
        "    if 'mix_labels' in inputs:\n",
        "      logging.info('Using mixup or cutmix!')\n",
        "      mix_label = self._one_hot(inputs['mix_labels'])\n",
        "      mix_ratio = inputs['ratio'][:, None]\n",
        "      label = mix_ratio * label + (1. - mix_ratio) * mix_label\n",
        "\n",
        "    # Apply label-smoothing to one-hot labels.\n",
        "    label_smoothing = self.config.training.label_smoothing\n",
        "    if not (label_smoothing >= 0. and label_smoothing < 1.):\n",
        "      raise ValueError(\n",
        "          \"'label_smoothing is {label_smoothing} and should be in [0, 1)\")\n",
        "    if label_smoothing > 0:\n",
        "      smooth_positives = 1. - label_smoothing\n",
        "      smooth_negatives = label_smoothing / self.config.data.num_classes\n",
        "      label = smooth_positives * label + smooth_negatives\n",
        "\n",
        "    loss_w_batch = utils.softmax_cross_entropy(logits, label)\n",
        "    loss = jnp.mean(loss_w_batch, dtype=loss_w_batch.dtype)\n",
        "    scaled_loss = loss / jax.device_count()\n",
        "\n",
        "    metrics = utils.topk_correct(logits, inputs['labels'], prefix='')\n",
        "    metrics = jax.tree_map(jnp.mean, metrics)\n",
        "\n",
        "    top_1_acc = metrics['top_1_acc']\n",
        "    top_5_acc = metrics['top_5_acc']\n",
        "\n",
        "    loss_scalars = dict(\n",
        "        loss=loss,\n",
        "        top_1_acc=top_1_acc,\n",
        "        top_5_acc=top_5_acc,\n",
        "    )\n",
        "\n",
        "    return scaled_loss, (loss_scalars, state)\n",
        "\n",
        "  def _update_func(\n",
        "      self,\n",
        "      params: hk.Params,\n",
        "      state: hk.State,\n",
        "      opt_state: OptState,\n",
        "      inputs: dataset.Batch,\n",
        "      rng: jnp.ndarray,\n",
        "      global_step: int,\n",
        "  ) -> Tuple[hk.Params, hk.State, OptState, Scalars]:\n",
        "    \"\"\"Applies an update to parameters and returns new state.\"\"\"\n",
        "    # This function computes the gradient of the first output of loss_fn and\n",
        "    # passes through the other arguments unchanged.\n",
        "    grad_loss_fn = jax.grad(self._loss_fn, has_aux=True)\n",
        "    scaled_grads, (loss_scalars, state) = grad_loss_fn(\n",
        "        params, state, inputs, rng)\n",
        "    grads = jax.lax.psum(scaled_grads, axis_name='i')\n",
        "\n",
        "    # Grab the learning rate to log before performing the step.\n",
        "    learning_rate = self._lr_schedule(global_step)\n",
        "\n",
        "    # Compute and apply updates via our optimizer.\n",
        "    updates, opt_state = self._optimizer.update(grads, opt_state, params)\n",
        "    params = optax.apply_updates(params, updates)\n",
        "\n",
        "    n_params = 0\n",
        "    for k in params.keys():\n",
        "      for l in params[k]:\n",
        "        n_params = n_params + np.prod(params[k][l].shape)\n",
        "\n",
        "    # Scalars to log (note: we log the mean across all hosts/devices).\n",
        "    scalars = {'learning_rate': learning_rate,\n",
        "               'n_params (M)': float(n_params/1e6),\n",
        "               'global_gradient_norm': optax.global_norm(grads)}\n",
        "    loss_scalars = {'train_{k}': v for k, v in loss_scalars.items()}\n",
        "    scalars.update(loss_scalars)\n",
        "    scalars = jax.lax.pmean(scalars, axis_name='i')\n",
        "\n",
        "    return params, state, opt_state, scalars\n",
        "\n",
        "  #                  _\n",
        "  #   _____   ____ _| |\n",
        "  #  / _ \\ \\ / / _` | |\n",
        "  # |  __/\\ V / (_| | |\n",
        "  #  \\___| \\_/ \\__,_|_|\n",
        "  #\n",
        "\n",
        "  def evaluate(self, global_step, rng, **unused_args):\n",
        "    \"\"\"See base class.\"\"\"\n",
        "    global_step = np.array(jl_utils.get_first(global_step))\n",
        "    scalars = jax.device_get(self._eval_epoch(jl_utils.get_first(rng)))\n",
        "\n",
        "    logging.info('[Step %d] Eval scalars: %s', global_step, scalars)\n",
        "    return scalars\n",
        "\n",
        "  def _eval_batch(\n",
        "      self,\n",
        "      params: hk.Params,\n",
        "      state: hk.State,\n",
        "      inputs: dataset.Batch,\n",
        "      rng: jnp.ndarray,\n",
        "  ) -> Scalars:\n",
        "    \"\"\"Evaluates a batch.\"\"\"\n",
        "    logits, _ = self.forward.apply(\n",
        "        params, state, rng, inputs, is_training=False)\n",
        "\n",
        "    labels = self._one_hot(inputs['labels'])\n",
        "    loss = utils.softmax_cross_entropy(logits, labels)\n",
        "\n",
        "    metrics = utils.topk_correct(logits, inputs['labels'], prefix='')\n",
        "    metrics = jax.tree_map(jnp.mean, metrics)\n",
        "    top_1_acc = metrics['top_1_acc']\n",
        "    top_5_acc = metrics['top_5_acc']\n",
        "\n",
        "    bs = logits.shape[0]\n",
        "\n",
        "    top_1_acc = jnp.expand_dims(top_1_acc, axis=0) * bs\n",
        "    top_5_acc = jnp.expand_dims(top_5_acc, axis=0) * bs\n",
        "\n",
        "    # NOTE: Returned values will be summed and finally divided by num_samples.\n",
        "    return {\n",
        "        'eval_loss': loss,\n",
        "        'eval_top_1_acc': top_1_acc, 'eval_top_5_acc': top_5_acc}\n",
        "\n",
        "  def _build_eval_input(self) -> Generator[dataset.Batch, None, None]:\n",
        "    split = dataset.Split.from_string(self.config.evaluation.subset)\n",
        "\n",
        "    return self._load_data(\n",
        "        split=split,\n",
        "        is_training=False,\n",
        "        batch_dims=[self.config.evaluation.batch_size])\n",
        "\n",
        "  def _eval_epoch(self, rng):\n",
        "    \"\"\"Evaluates an epoch.\"\"\"\n",
        "    num_samples = 0.\n",
        "    summed_scalars = None\n",
        "\n",
        "    params = jl_utils.get_first(self._params)\n",
        "    state = jl_utils.get_first(self._state)\n",
        "\n",
        "    for inputs in self._build_eval_input():\n",
        "      num_samples += inputs['labels'].shape[0]\n",
        "      scalars = self._eval_batch(params, state, inputs, rng)\n",
        "\n",
        "      # Accumulate the sum of scalars for each step.\n",
        "      scalars = jax.tree_map(lambda x: jnp.sum(x, axis=0), scalars)\n",
        "      if summed_scalars is None:\n",
        "        summed_scalars = scalars\n",
        "      else:\n",
        "        summed_scalars = jax.tree_multimap(jnp.add, summed_scalars, scalars)\n",
        "\n",
        "    mean_scalars = jax.tree_map(lambda x: x / num_samples, summed_scalars)\n",
        "    return mean_scalars\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  # sys.argv[0] = \"./perceiver/train/experiment.py\"\n",
        "  #sys.argv[1] = \"./perceiver/train/experiment.py\"\n",
        "  flags.mark_flag_as_required('config')\n",
        "  app.run(functools.partial(platform.main, Experiment))\n"
      ],
      "metadata": {
        "id": "mvliCnjWycYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! python perceiver/train/experiment.py --config=perceiver/train/experiment.py --logtostderr\n"
      ],
      "metadata": {
        "id": "LUge1YSk30DH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.compat.v1.flags.DEFINE_string('f','','')"
      ],
      "metadata": {
        "id": "50Gk6QGc6_E_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jupyter-console \n",
        "!pip install absl-py\n"
      ],
      "metadata": {
        "id": "MyCKnw1f5GrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 777 /content/perceiver/train/launch_local.sh\n",
        "!bash ./perceiver/train/launch_local.sh"
      ],
      "metadata": {
        "id": "tZ7gHkom0QU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ../"
      ],
      "metadata": {
        "id": "DKzr80sa1EgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%run \"/content/perceiver/io_processors.py\"\n",
        "%run  \"/content/perceiver/perceiver.py\"\n",
        "%run \"/content/perceiver/position_encoding.py\"\n",
        "%run \"/content/perceiver/train/dataset.py\""
      ],
      "metadata": {
        "id": "ryrJ9F_Y2z1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/perceiver')\n",
        "sys.path.append('/content/perceiver/train')\n",
        "\n",
        "import io_processors"
      ],
      "metadata": {
        "id": "O-SvL5LM3cGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install altair"
      ],
      "metadata": {
        "id": "ZcdqJ70B4oHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python ./perceiver/train/experiment.py --config=./perceiver/train/experiment.py --logtostderr\n"
      ],
      "metadata": {
        "id": "1_LMQcza0zpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-addons"
      ],
      "metadata": {
        "id": "Ac5oMMoQ0Ef8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Mine \n"
      ],
      "metadata": {
        "id": "obq7ZJJJePmQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import enum\n",
        "from typing import Any, Generator, Mapping, Optional, Sequence, Text, Tuple\n",
        "\n",
        "import jax\n",
        "import numpy as np\n",
        "import tensorflow.compat.v2 as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "from train import autoaugment\n",
        "\n",
        "\n",
        "Batch = Mapping[Text, np.ndarray]\n",
        "MEAN_RGB = (0.485 * 255, 0.456 * 255, 0.406 * 255)\n",
        "STDDEV_RGB = (0.229 * 255, 0.224 * 255, 0.225 * 255)\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "INPUT_DIM = 224  # The number of pixels in the image resize.\n",
        "\n",
        "\n",
        "class Split(enum.Enum):\n",
        "  \"\"\"ImageNet dataset split.\"\"\"\n",
        "  TRAIN = 1\n",
        "  TRAIN_AND_VALID = 2\n",
        "  VALID = 3\n",
        "  TEST = 4\n",
        "\n",
        "  @classmethod\n",
        "  def from_string(cls, name: Text) -> 'Split':\n",
        "    return {'TRAIN': Split.TRAIN, 'TRAIN_AND_VALID': Split.TRAIN_AND_VALID,\n",
        "            'VALID': Split.VALID, 'VALIDATION': Split.VALID,\n",
        "            'TEST': Split.TEST}[name.upper()]\n",
        "\n",
        "  @property\n",
        "  def num_examples(self):\n",
        "    return {Split.TRAIN_AND_VALID: 1281167, Split.TRAIN: 1271167,\n",
        "            Split.VALID: 10000, Split.TEST: 50000}[self]\n",
        "\n",
        "\n",
        "def load(\n",
        "    split: Split,\n",
        "    *,\n",
        "    is_training: bool,\n",
        "    # batch_dims should be:\n",
        "    # [device_count, per_device_batch_size] or [total_batch_size]\n",
        "    batch_dims: Sequence[int],\n",
        "    augmentation_settings: Mapping[str, Any],\n",
        "    # The shape to which images are resized.\n",
        "    im_dim: int = INPUT_DIM,\n",
        "    threadpool_size: int = 48,\n",
        "    max_intra_op_parallelism: int = 1,\n",
        ") -> Generator[Batch, None, None]:\n",
        "  \"\"\"Loads the given split of the dataset.\"\"\"\n",
        "  start, end = _shard(split, jax.host_id(), jax.host_count())\n",
        "\n",
        "  im_size = (im_dim, im_dim)\n",
        "\n",
        "  total_batch_size = np.prod(batch_dims)\n",
        "\n",
        "  tfds_split = tfds.core.ReadInstruction(_to_tfds_split(split),\n",
        "                                         from_=start, to=end, unit='abs')\n",
        "\n",
        "  ds = tfds.load('imagenet2012:5.*.*', split=tfds_split,\n",
        "                 decoders={'image': tfds.decode.SkipDecoding()})\n",
        "\n",
        "  options = tf.data.Options()\n",
        "  options.experimental_threading.private_threadpool_size = threadpool_size\n",
        "  options.experimental_threading.max_intra_op_parallelism = (\n",
        "      max_intra_op_parallelism)\n",
        "  options.experimental_optimization.map_parallelization = True\n",
        "  if is_training:\n",
        "    options.experimental_deterministic = False\n",
        "  ds = ds.with_options(options)\n",
        "\n",
        "  if is_training:\n",
        "    if jax.host_count() > 1:\n",
        "      # Only cache if we are reading a subset of the dataset.\n",
        "      ds = ds.cache()\n",
        "    ds = ds.repeat()\n",
        "    ds = ds.shuffle(buffer_size=10 * total_batch_size, seed=0)\n",
        "\n",
        "  else:\n",
        "    if split.num_examples % total_batch_size != 0:\n",
        "      raise ValueError(f'Test/valid must be divisible by {total_batch_size}')\n",
        "\n",
        "  def crop_augment_preprocess(example):\n",
        "    image, _ = _preprocess_image(\n",
        "        example['image'], is_training, im_size, augmentation_settings)\n",
        "\n",
        "    label = tf.cast(example['label'], tf.int32)\n",
        "\n",
        "    out = {'images': image, 'labels': label}\n",
        "\n",
        "    if is_training:\n",
        "      if augmentation_settings['cutmix']:\n",
        "        out['mask'] = cutmix_padding(*im_size)\n",
        "        out['cutmix_ratio'] = tf.reduce_mean(out['mask'])\n",
        "      if augmentation_settings['mixup_alpha'] is not None:\n",
        "        beta = tfp.distributions.Beta(\n",
        "            augmentation_settings['mixup_alpha'],\n",
        "            augmentation_settings['mixup_alpha'])\n",
        "        out['mixup_ratio'] = beta.sample()\n",
        "    return out\n",
        "\n",
        "  ds = ds.map(crop_augment_preprocess, num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "  # Mixup/cutmix by temporarily batching (using the per-device batch size):\n",
        "  use_cutmix = augmentation_settings['cutmix']\n",
        "  use_mixup = augmentation_settings['mixup_alpha'] is not None\n",
        "  if is_training and (use_cutmix or use_mixup):\n",
        "    inner_batch_size = batch_dims[-1]\n",
        "    # Apply mixup, cutmix, or mixup + cutmix on batched data.\n",
        "    # We use data from 2 batches to produce 1 mixed batch.\n",
        "    ds = ds.batch(inner_batch_size * 2)\n",
        "    if not use_cutmix and use_mixup:\n",
        "      ds = ds.map(my_mixup, num_parallel_calls=AUTOTUNE)\n",
        "    elif use_cutmix and not use_mixup:\n",
        "      ds = ds.map(my_cutmix, num_parallel_calls=AUTOTUNE)\n",
        "    elif use_cutmix and use_mixup:\n",
        "      ds = ds.map(my_mixup_cutmix, num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "    # Unbatch for further processing.\n",
        "    ds = ds.unbatch()\n",
        "\n",
        "  for batch_size in reversed(batch_dims):\n",
        "    ds = ds.batch(batch_size)\n",
        "\n",
        "  ds = ds.prefetch(AUTOTUNE)\n",
        "\n",
        "  yield from tfds.as_numpy(ds)\n",
        "\n",
        "\n",
        "def _shard(\n",
        "    split: Split, shard_index: int, num_shards: int) -> Tuple[int, int]:\n",
        "  \"\"\"Returns [start, end) for the given shard index.\"\"\"\n",
        "  assert shard_index < num_shards\n",
        "  arange = np.arange(split.num_examples)\n",
        "  shard_range = np.array_split(arange, num_shards)[shard_index]\n",
        "  start, end = shard_range[0], (shard_range[-1] + 1)\n",
        "  if split == Split.TRAIN:\n",
        "    # Note that our TRAIN=TFDS_TRAIN[10000:] and VALID=TFDS_TRAIN[:10000].\n",
        "    offset = Split.VALID.num_examples\n",
        "    start += offset\n",
        "    end += offset\n",
        "  return start, end\n",
        "\n",
        "\n",
        "def _preprocess_image(\n",
        "    image_bytes: tf.Tensor,\n",
        "    is_training: bool,\n",
        "    image_size: Sequence[int],\n",
        "    augmentation_settings: Mapping[str, Any],\n",
        ") -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "  \"\"\"Returns processed and resized images.\"\"\"\n",
        "\n",
        "  # Get the image crop.\n",
        "  if is_training:\n",
        "    image, im_shape = _decode_and_random_crop(image_bytes)\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "  else:\n",
        "    image, im_shape = _decode_and_center_crop(image_bytes)\n",
        "  assert image.dtype == tf.uint8\n",
        "\n",
        "  # Optionally apply RandAugment: https://arxiv.org/abs/1909.13719\n",
        "  if is_training:\n",
        "    if augmentation_settings['randaugment'] is not None:\n",
        "      # Input and output images are dtype uint8.\n",
        "      image = autoaugment.distort_image_with_randaugment(\n",
        "          image,\n",
        "          num_layers=augmentation_settings['randaugment']['num_layers'],\n",
        "          magnitude=augmentation_settings['randaugment']['magnitude'])\n",
        "\n",
        "  # Resize and normalize the image crop.\n",
        "  # NOTE: Bicubic resize (1) casts uint8 to float32 and (2) resizes without\n",
        "  # clamping overshoots. This means values returned will be outside the range\n",
        "  # [0.0, 255.0] (e.g. we have observed outputs in the range [-51.1, 336.6]).\n",
        "  image = tf.image.resize(\n",
        "      image, image_size, tf.image.ResizeMethod.BICUBIC)\n",
        "  image = _normalize_image(image)\n",
        "\n",
        "  return image, im_shape\n",
        "\n",
        "##########################################################\n",
        " #Utilities to open video files using CV2\n",
        "def crop_center_square(frame):\n",
        "  y, x = frame.shape[0:2]\n",
        "  min_dim = min(y, x)\n",
        "  start_x = (x // 2) - (min_dim // 2)\n",
        "  start_y = (y // 2) - (min_dim // 2)\n",
        "  return frame[start_y:start_y+min_dim,start_x:start_x+min_dim]\n",
        "\n",
        "def load_video(path, max_frames=0, resize=(224, 224)):\n",
        "  cap = cv2.VideoCapture(path)\n",
        "  frames = []\n",
        "  try:\n",
        "    while True:\n",
        "      ret, frame = cap.read()\n",
        "      if not ret:\n",
        "        break\n",
        "      frame = crop_center_square(frame)\n",
        "      frame = cv2.resize(frame, resize)\n",
        "      frame = frame[:, :, [2, 1, 0]]\n",
        "      frames.append(frame)\n",
        "      \n",
        "      if len(frames) == max_frames:\n",
        "        break\n",
        "  finally:\n",
        "    cap.release()\n",
        "  return np.array(frames) / 255.0\n",
        "############################################\n",
        "\n",
        "def _normalize_image(image: tf.Tensor) -> tf.Tensor:\n",
        "  \"\"\"Normalize the image to zero mean and unit variance.\"\"\"\n",
        "  image -= tf.constant(MEAN_RGB, shape=[1, 1, 3], dtype=image.dtype)\n",
        "  image /= tf.constant(STDDEV_RGB, shape=[1, 1, 3], dtype=image.dtype)\n",
        "  return image\n",
        "\n",
        "\n",
        "\n",
        "def _decode_whole_image(image_bytes: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "  image = tf.io.decode_jpeg(image_bytes, channels=3)\n",
        "  im_shape = tf.io.extract_jpeg_shape(image_bytes, output_type=tf.int32)\n",
        "  return image, im_shape\n",
        "\n",
        "\n",
        "def _center_crop(image, crop_dim):\n",
        "  \"\"\"Center crops an image to a target dimension.\"\"\"\n",
        "  image_height = image.shape[0]\n",
        "  image_width = image.shape[1]\n",
        "  offset_height = ((image_height - crop_dim) + 1) // 2\n",
        "  offset_width = ((image_width - crop_dim) + 1) // 2\n",
        "  return tf.image.crop_to_bounding_box(\n",
        "      image, offset_height, offset_width, crop_dim, crop_dim)\n",
        "\n",
        "\n",
        "def _decode_and_center_crop(\n",
        "    image_bytes: tf.Tensor,\n",
        "    jpeg_shape: Optional[tf.Tensor] = None,\n",
        ") -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "  \"\"\"Crops to center of image with padding then scales.\"\"\"\n",
        "  if jpeg_shape is None:\n",
        "    if image_bytes.dtype == tf.dtypes.string:\n",
        "      jpeg_shape = tf.image.extract_jpeg_shape(image_bytes)\n",
        "    else:\n",
        "      jpeg_shape = tf.shape(image_bytes)\n",
        "\n",
        "  image_height = jpeg_shape[0]\n",
        "  image_width = jpeg_shape[1]\n",
        "\n",
        "  padded_center_crop_size = tf.cast(\n",
        "      ((INPUT_DIM / (INPUT_DIM + 32)) *\n",
        "       tf.cast(tf.minimum(image_height, image_width), tf.float32)), tf.int32)\n",
        "\n",
        "  offset_height = ((image_height - padded_center_crop_size) + 1) // 2\n",
        "  offset_width = ((image_width - padded_center_crop_size) + 1) // 2\n",
        "  crop_window = [offset_height, offset_width,\n",
        "                 padded_center_crop_size, padded_center_crop_size]\n",
        "\n",
        "  if image_bytes.dtype == tf.dtypes.string:\n",
        "    image = tf.image.decode_and_crop_jpeg(image_bytes,\n",
        "                                          tf.stack(crop_window),\n",
        "                                          channels=3)\n",
        "  else:\n",
        "    image = tf.image.crop_to_bounding_box(image_bytes, *crop_window)\n",
        "\n",
        "  im_shape = tf.stack([padded_center_crop_size, padded_center_crop_size])\n",
        "  return image, im_shape\n",
        "\n"
      ],
      "metadata": {
        "id": "zgxLgK19eRs_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Perceiver IO: Video Autoencoding_ME.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}